# MoE Optimizer - Python Package Requirements

# ============ Core Dependencies (Required) ============
# These are needed for basic functionality

# Will install when we have GPU access:
# torch>=2.1.0
# transformers>=4.35.0
# accelerate>=0.24.0

# ============ Optimization Libraries (Install on GPU) ============
# vllm>=0.3.0  # Main serving engine
# flash-attn>=2.3.0  # Attention optimization
# triton>=2.1.0  # CUDA kernel compilation

# ============ Optional (For FP8 on H100) ============
# transformer-engine  # NVIDIA FP8 support
# Install: pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable

# ============ Utilities (Can install now) ============
pyyaml>=6.0  # Config file support
numpy>=1.24.0
pandas>=2.0.0
tqdm>=4.65.0

# ============ Monitoring & Metrics ============
prometheus-client>=0.19.0  # Metrics export
flask>=3.0.0  # Metrics server

# ============ Profiling & Development ============
pytest>=7.4.0  # Testing
pytest-cov>=4.1.0  # Coverage
black>=23.0.0  # Code formatting
mypy>=1.5.0  # Type checking

# ============ Visualization (Optional) ============
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.17.0
tabulate>=0.9.0  # Pretty tables

# ============ Documentation (Optional) ============
# sphinx>=7.0.0
# sphinx-rtd-theme>=1.3.0

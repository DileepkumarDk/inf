# Conservative Configuration
# High accuracy, lower performance
# Good for: Initial testing, accuracy validation

model_path: "YOUR_MODEL_HERE"  # Set via --model or replace this
model_type: "moe"
num_gpus: 1  # FIX #10: Single GPU for Qwen3-30B
num_experts: 128  # FIX #10: Qwen3-30B-A3B has 128 experts
experts_per_token: 8  # FIX #10: Top-8 routing
max_model_len: 4096

# Quantization (conservative)
enable_fp8: true
fp8_router_precision: "fp16"  # Keep router in FP16 for accuracy

# Batch processing (conservative for Qwen3)
enable_dual_batch_overlap: true
max_num_batched_tokens: 16384  # FIX #10: Moderate increase for 128 experts
max_num_seqs: 512  # FIX #10: Balanced parallelism

# Advanced optimizations (disabled for safety)
enable_disaggregation: false
enable_kv_tiering: false
enable_expert_placement: false
enable_expert_sparsity: false

# Safety settings
gpu_memory_utilization: 0.85  # More headroom
enable_cuda_graphs: true
enable_profiling: false
enable_metrics: true
metrics_port: 9090
log_level: "INFO"

# Fallback and safety
enable_fallback_path: true
max_retries: 3
timeout_seconds: 30.0

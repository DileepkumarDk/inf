# Single H100 Configuration
# Optimized for 4B-7B MoE models on 1Ã— H100 SXM (80GB)
# Perfect for: DeepSeek-Coder-6.7B, Phi-3.5-MoE-Instruct, small Mixtral models

model_path: "REPLACE_WITH_YOUR_MODEL"  # e.g., "Qwen/Qwen3-30B-A3B" or "deepseek-ai/deepseek-coder-6.7b-instruct"
model_type: "moe"
num_gpus: 1  # Single H100 (80GB - perfect for Qwen3-30B)
num_experts: 128  # FIX #10: Default for Qwen3-30B-A3B (adjust for other models)
experts_per_token: 8  # FIX #10: Top-8 for Qwen3
max_model_len: 4096

# Quantization (FP8 enabled - H100 native)
enable_fp8: true
fp8_router_precision: "fp16"  # Keep router accurate

# Batch processing (optimized for single GPU with 128 experts)
enable_dual_batch_overlap: true
max_num_batched_tokens: 32768  # FIX #10: Increased for Qwen3's 128 experts
max_num_seqs: 1024  # FIX #10: High concurrency for many experts

# Advanced optimizations
enable_disaggregation: false  # Requires 2+ GPUs
enable_kv_tiering: true  # Works on single GPU
kv_recent_window: 2048
kv_cache_dtype_recent: "fp16"
kv_cache_dtype_old: "fp8"

enable_expert_placement: true  # Still useful for cache locality
# experts_per_token already set above (line 5)

enable_expert_sparsity: false  # Optional - requires fine-tuning

# Single GPU settings
gpu_memory_utilization: 0.90  # Single GPU - use more memory
enable_cuda_graphs: true
enable_profiling: false
enable_metrics: true
metrics_port: 9090
log_level: "INFO"

# Safety
enable_fallback_path: true
max_retries: 3
timeout_seconds: 30.0
